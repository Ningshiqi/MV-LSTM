# /usr/bin/env python
# -*- coding;utf-8 -*-

"""
处理xml问答数据

"""

import os
import xml.dom.minidom as xmldom
import tensorflow as tf



# def generate_data():
#
#     answer=list()
#     file_name = 'manner.xml'
#     data_path = os.path.abspath(file_name)
#     #文档对象
#     domobj = xmldom.parse(data_path)
#     #得到元素对象
#     elementobj = domobj.documentElement
#     #获得子标签
#     subElementObj = elementobj.getElementsByTagName('vespaadd')
#     for sub in subElementObj:
#         answer.append(len(sub.getElementsByTagName('nbestanswers')[0].getElementsByTagName('answer_item')))
#     print(answer)
#     print("Length:",len(answer))
#     print("Min:",min(answer))
#     print("Max:",max(answer))

"""
Length: 142627
Min: 1
Max: 2235
"""
def ranking_loss1(y_pred):
# for index in range(neg_sample_num):
#    hinge_loss = 1-y_pred[::neg_sample_num+1]+y_pred[index+1::neg_sample_num+1]
# loss = 1.0*(self.config.unit_size-1)+tf.reduce_sum(y_pred,1)-5*y_pred[:,0]
# loss = tf.reduce_mean(tf.maximum(0.0,loss))
    sum_loss = tf.Variable(0.0)
    neg_num = 4
    for index in range(neg_num):
        tmp = tf.maximum(0.0, 1.0 + y_pred[:, index + 1] - y_pred[:, 0])
        sum_loss += tf.reduce_mean(tmp)
    return sum_loss / neg_num



def ranking_loss2(y_pred):

    neg_num = 4
    loss1 = tf.reduce_mean(tf.maximum(0.0,1.0+y_pred[:,1]-y_pred[:,0]))
    loss2 = tf.reduce_mean(tf.maximum(0.0,1.0+y_pred[:,2]-y_pred[:,0]))
    loss3 = tf.reduce_mean(tf.maximum(0.0,1.0+y_pred[:,3]-y_pred[:,0]))
    loss4 = tf.reduce_mean(tf.maximum(0.0,1.0+y_pred[:,4]-y_pred[:,0]))
    loss = tf.stack([loss1,loss2,loss3,loss4],axis=0)

    return tf.reduce_mean(loss)









if __name__ == '__main__':
    #generate_data()
    sess = tf.Session()
    arr = [[9.9994886e-01 ,9.9995315e-01, 9.9994981e-01, 9.9994528e-01, 9.9995100e-01],
 [9.9995422e-01, 9.9995410e-01, 9.9996662e-01, 9.9993742e-01, 9.9995410e-01],
 [9.9994493e-01, 9.9994957e-01, 9.9994898e-01, 9.9994910e-01, 9.9995065e-01],
 [9.9995399e-01, 9.9995244e-01, 9.9995291e-01, 1.8501485e-07, 1.5761358e-07],
 [9.9995208e-01, 9.9995160e-01, 9.9995077e-01, 2.2196974e-07, 9.9995422e-01],
 [9.9994349e-01, 9.9994493e-01, 9.9994540e-01, 9.9994564e-01, 9.9994493e-01],
 [9.9995029e-01, 9.9995065e-01, 9.9995232e-01, 9.9993658e-01, 9.9995244e-01],
 [9.9995780e-01, 9.9995732e-01, 9.9995780e-01, 9.9995208e-01, 9.9996841e-01],
 [9.9995685e-01, 9.9995470e-01, 9.9995315e-01, 9.9995339e-01, 9.9995053e-01],
 [9.9995291e-01, 9.9994767e-01, 9.9996603e-01, 9.9995315e-01, 9.9995339e-01],
 [9.9995601e-01, 1.8372393e-07, 9.9995518e-01, 9.9995220e-01, 9.9995685e-01],
 [9.9995148e-01, 9.9995148e-01, 9.9993718e-01, 1.8824015e-07, 1.8948330e-07],
 [9.9994648e-01, 9.9994695e-01, 2.1750860e-07, 9.9993527e-01, 9.9994409e-01],
 [9.9995327e-01, 9.9994791e-01, 9.9994373e-01, 9.9994981e-01, 9.9995041e-01],
 [9.9995244e-01, 9.9995089e-01, 9.9995100e-01, 9.9995160e-01, 9.9995434e-01],
 [9.9995196e-01, 9.9995148e-01, 9.9995196e-01, 9.9995351e-01, 2.0099783e-07],
 [9.9995434e-01, 9.9994981e-01, 9.9995351e-01, 9.9995446e-01, 9.9995482e-01],
 [9.9995363e-01, 1.8205930e-07, 9.9995589e-01, 1.9523189e-07, 9.9995124e-01],
 [9.9994969e-01, 9.9995291e-01, 9.9995303e-01, 9.9995351e-01, 9.9995291e-01],
 [9.9994969e-01, 9.9994349e-01, 2.3893020e-07, 2.3917528e-07, 2.0661324e-07],
 [9.9995053e-01, 9.9994779e-01, 9.9994814e-01, 9.9994409e-01, 9.9994838e-01],
 [9.9994898e-01, 9.9994695e-01, 9.9994874e-01, 9.9994946e-01, 2.5018591e-07],
 [9.9995160e-01, 2.1337769e-07, 9.9994993e-01, 9.9994898e-01, 2.1604966e-07],
 [9.9995387e-01, 9.9995530e-01, 9.9995410e-01, 9.9995470e-01, 9.9995172e-01],
 [9.9995017e-01, 9.9995065e-01, 2.3280164e-07, 9.9995136e-01, 2.2760427e-07],
 [9.9995065e-01, 9.9995065e-01, 2.0456186e-07, 9.9994874e-01, 2.3601510e-07],
 [9.9994361e-01, 1.7349338e-07, 2.3193095e-07, 1.9291156e-07, 9.9994481e-01],
 [9.9994874e-01, 9.9994922e-01, 9.9996698e-01, 1.6947149e-07, 1.8241521e-07],
 [9.9995184e-01, 9.9995160e-01, 2.0272464e-07, 2.4063266e-07, 9.9995077e-01],
 [9.9994957e-01, 9.9995089e-01, 1.7859243e-07, 9.9995291e-01, 9.9994195e-01],
 [9.9994862e-01, 9.9994922e-01, 9.9995017e-01, 9.9994564e-01, 2.1112680e-07],
 [9.9995184e-01, 2.1272244e-07, 1.8679449e-07, 2.1072610e-07, 2.2025687e-07],
 [9.9995148e-01, 1.6625545e-07, 2.0020984e-07, 1.9503035e-07, 2.0307255e-07],
 [9.9995208e-01, 9.9995172e-01, 9.9995446e-01, 9.9994195e-01, 9.9995363e-01],
 [9.9995124e-01, 9.9995279e-01, 9.9994743e-01, 9.9995220e-01, 9.9995196e-01],
 [9.9994552e-01, 2.0565999e-07, 2.3915064e-07, 9.9994743e-01, 9.9994767e-01],
 [9.9994969e-01, 9.9995065e-01, 9.9995315e-01, 1.9068335e-07, 1.8173905e-07],
 [9.9995434e-01, 9.9995184e-01, 9.9995542e-01, 9.9995387e-01, 9.9995148e-01],
 [9.9995255e-01, 9.9995029e-01, 2.1322126e-07, 2.2076242e-07, 9.9995065e-01],
 [9.9995458e-01, 1.7979414e-07, 9.9995279e-01 ,1.5930459e-07, 9.9995375e-01],
 [9.9995077e-01, 9.9995077e-01, 9.9994946e-01, 9.9995065e-01, 9.9996471e-01],
 [9.9995315e-01, 9.9994397e-01, 9.9995065e-01, 9.9995351e-01, 1.9923311e-07],
 [9.9995267e-01, 1.6843224e-07, 1.8272583e-07, 2.0310509e-07, 9.9994993e-01],
 [9.9995267e-01, 9.9995124e-01, 9.9995065e-01, 9.9995077e-01, 9.9994445e-01],
 [9.9995327e-01, 9.9995399e-01, 9.9994385e-01, 1.3807085e-07, 9.9994636e-01],
 [9.9995077e-01, 2.1894482e-07, 2.2837396e-07, 2.0443255e-07, 9.9994516e-01],
 [9.9995518e-01, 9.9995255e-01, 9.9995625e-01, 9.9995625e-01, 9.9995208e-01],
 [9.9995375e-01, 9.9995148e-01, 1.7620869e-07, 1.7726279e-07, 9.9995351e-01],
 [9.9995184e-01, 1.8024497e-07, 1.8000001e-07, 1.8918357e-07, 1.9307130e-07],
 [9.9995077e-01, 9.9994445e-01, 9.9995065e-01, 9.9995029e-01, 9.9994886e-01],
 [9.9994993e-01, 9.9994838e-01, 9.9995649e-01, 2.1968376e-07, 9.9995267e-01],
 [9.9995112e-01, 9.9995124e-01, 2.3649230e-07, 9.9994910e-01, 2.0659432e-07],
 [9.9995148e-01, 9.9995124e-01, 1.8414492e-07, 9.9995041e-01, 9.9995065e-01],
 [9.9995375e-01, 2.0728547e-07, 2.0597994e-07, 2.1623950e-07, 9.9994886e-01],
 [9.9994755e-01, 9.9994743e-01, 9.9994862e-01, 2.5491343e-07, 9.9994171e-01],
 [9.9995577e-01, 9.9995661e-01, 9.9995494e-01, 1.7497688e-07, 1.5197645e-07],
 [9.9995208e-01, 9.9994981e-01, 2.5136256e-07, 9.9994946e-01, 9.9994814e-01],
 [9.9995220e-01, 1.8076727e-07, 9.9995255e-01, 1.5419317e-07, 1.5747025e-07],
 [9.9994838e-01, 9.9993789e-01, 2.5177667e-07, 9.9994993e-01, 2.2459761e-07],
 [9.9995375e-01, 9.9995124e-01, 1.8363669e-07, 9.9995363e-01 ,9.9993718e-01],
 [9.9995685e-01, 9.9995601e-01, 9.9995399e-01, 9.9995339e-01, 1.5763013e-07],
 [9.9995100e-01, 2.1582889e-07, 9.9994850e-01, 9.9995184e-01, 2.3493627e-07],
 [9.9993825e-01, 9.9994206e-01, 1.8405854e-07, 2.3170500e-07, 1.8618164e-07],
 [9.9995220e-01, 9.9995077e-01, 9.9995255e-01, 9.9993849e-01, 9.9995053e-01]]

    y_pred = tf.Variable(arr)
    loss1 = ranking_loss1(y_pred)
    loss2 = ranking_loss2(y_pred)
    sess.run(tf.global_variables_initializer())

    print(sess.run(loss1))
    print(sess.run(loss2))





